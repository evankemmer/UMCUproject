{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fdc937c-8ab8-4a58-bb3f-6718bd13ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io, math, os\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from nltk.corpus import cmudict\n",
    "from numpy import linalg as LA\n",
    "import seaborn as sns\n",
    "import pylab\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import Levenshtein\n",
    "from sklearn.cluster import AgglomerativeClustering, AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "332bca10-d8f5-41ac-be5c-4748e51f12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['F1', 'F5', 'M1', 'M3']\n",
    "subj_index = 0\n",
    "\n",
    "# for determining the number of syllables \n",
    "d = cmudict.dict()\n",
    "\n",
    "# TODO: put this in utils file \n",
    "def get_srate(file_number):\n",
    "    directory = 'data/Data/{}/mat'.format(subjects[subj_index])\n",
    "    \n",
    "    files = sorted(os.listdir(directory))\n",
    "    try:\n",
    "        files.remove('.DS_Store')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    file = files[file_number]\n",
    "    \n",
    "    f = os.path.join(directory, file)\n",
    "    mat = scipy.io.loadmat(f)['usctimit_ema_{}_{:03}_{:03}'.format(subjects[subj_index].lower(), file_number*5 + 1, file_number*5 + 5)]\n",
    "    \n",
    "    #returns the srate which is awkwardly stored here\n",
    "    return mat[0][1][1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c68a21-faf0-4b99-b340-934b9a7952d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data/Data/{}/mat'.format(subjects[subj_index])\n",
    "counter = 1\n",
    "UL_df, LL_df, JW_df, TD_df, TB_df, TT_df = [], [], [], [], [], []\n",
    "\n",
    "for filename in sorted(os.listdir(directory)):\n",
    "    if filename.endswith('.mat'):\n",
    "        f = os.path.join(directory, filename)\n",
    "        mat = scipy.io.loadmat(f)\n",
    "        # takes the data that is stored at the key that precedes the data for each .mat file\n",
    "        data = mat['usctimit_ema_{}_{:03}_{:03}'.format(subjects[subj_index].lower(), counter, counter + 4)]\n",
    "        counter += 5\n",
    "\n",
    "        # make dataframes of the six positions\n",
    "        UL_df.append(pd.DataFrame.from_dict(data[0][1][2]))\n",
    "        LL_df.append(pd.DataFrame.from_dict(data[0][2][2]))\n",
    "        JW_df.append(pd.DataFrame.from_dict(data[0][3][2]))\n",
    "        TD_df.append(pd.DataFrame.from_dict(data[0][4][2]))\n",
    "        TB_df.append(pd.DataFrame.from_dict(data[0][5][2]))\n",
    "        TT_df.append(pd.DataFrame.from_dict(data[0][6][2]))\n",
    "        \n",
    "dataframes = [UL_df, LL_df, JW_df, TB_df, TD_df, TT_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7028f81b-0dbd-4ef5-9369-e4dc64fa907b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jz/414l_qjs6s1bndtz3ppcy8cc0000gn/T/ipykernel_47285/2198873938.py:25: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  df.syl = nsyl(split_line[1], d)\n"
     ]
    }
   ],
   "source": [
    "frames = {}\n",
    "sensors = ['ULx', 'ULy', 'LLx', 'LLy', \n",
    "           'JWx', 'JWy', 'TDx', 'TDy', \n",
    "           'TBx', 'TBy', 'TTx', 'TTy']\n",
    "\n",
    "with open('timestamps.txt', 'r') as file:\n",
    "    timestamps = file.read().splitlines()\n",
    "    for word_number, line in enumerate(timestamps):\n",
    "        split_line = line.split(',')\n",
    "        sent_number = int(split_line[-1])\n",
    "        \n",
    "        # find start and end by multiplying the timestamps with the sampling rate\n",
    "        starting_point = math.floor(float(split_line[2]) * get_srate(int(split_line[0])))\n",
    "        end_point = math.ceil(float(split_line[3]) * get_srate(int(split_line[0])))\n",
    "        \n",
    "        # make new dataframe for the current word\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        for sensor in sensors:\n",
    "            # position, dimension, file_number, starting_point, end_point\n",
    "            array = get_pos_list(sensor[:2], sensor[-1], int(split_line[0]), starting_point, end_point, dataframes)\n",
    "            df[sensor] = pd.Series(array)\n",
    "            df.word = split_line[1]\n",
    "            # TODO: get rid of UserWarning \n",
    "            df.sent = int(split_line[-1])\n",
    "            df.syl = nsyl(split_line[1], d)\n",
    "            frames[word_number] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6b193f5-49b5-47d1-bbc4-f1bb39707702",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_normalize = True\n",
    "\n",
    "syl1_words, syl2_words, syl3_words = {}, {}, {}\n",
    "syl4_words, syl5_words, syl6_words = {}, {}, {}\n",
    "\n",
    "syl_frames = [syl1_words, syl2_words, \n",
    "              syl3_words, syl4_words, \n",
    "              syl5_words, syl6_words]\n",
    "\n",
    "# fit scaler on a global level to prevent local standardization \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat(frames))\n",
    "\n",
    "for i in range(1, len(syl_frames) + 1):\n",
    "    for count, frame in enumerate(frames):\n",
    "        # for some reason some words store the number of syllables in an array, hence:\n",
    "        if (isinstance(frames[frame].syl, list) and frames[frame].syl[0] == i) or\\\n",
    "            (not isinstance(frames[frame].syl, list) and frames[frame].syl == i):\n",
    "            # standardize the data to have a mean of 0 and approx. a SD of 1\n",
    "            if do_normalize:\n",
    "                data = scaler.transform(frames[frame])\n",
    "                df = pd.DataFrame(data, columns=sensors)\n",
    "                syl_frames[i - 1][count] = df\n",
    "                \n",
    "                # set meta-data, at this point we only need the word and the sentence it came from\n",
    "                syl_frames[i - 1][count].word = frames[frame].word\n",
    "                syl_frames[i - 1][count].sent = frames[frame].sent\n",
    "            else:\n",
    "                syl_frames[i - 1][count] = frames[frame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c5f7f10-be3a-41e1-8b04-afb51384289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding \n",
    "# just an observation: running this code twice returns an exception, will look into it later \n",
    "for i, frame in enumerate(syl_frames):\n",
    "    # target length is the the word with the most samples in that syllable category\n",
    "    target_length = longest_frame(frame)\n",
    "    for word in frame.keys():\n",
    "        current_length = frame[word].shape[0]\n",
    "        pad_length1 = int((target_length - current_length) / 2)\n",
    "        if np.mod((target_length - current_length),2) == 1:\n",
    "            pad_length2 = pad_length1 + 1\n",
    "        else:\n",
    "            pad_length2 = pad_length1\n",
    "\n",
    "        frame[word] = pd.DataFrame(np.pad(frame[word].values, \n",
    "                                   ((pad_length1,pad_length2), (0,0)), 'mean'),\n",
    "                                   columns=sensors).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7191761c-479b-4655-b9ba-e3ac8afc9756",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_syl1, labels_syl2, labels_syl3 = [], [], []\n",
    "labels_syl4, labels_syl5, labels_syl6 = [], [], []\n",
    "\n",
    "labels = [labels_syl1, labels_syl2, \n",
    "          labels_syl3, labels_syl4, \n",
    "          labels_syl5, labels_syl6]\n",
    "\n",
    "for i, frame in enumerate(syl_frames):\n",
    "    for word in frame:\n",
    "        labels[i].append(frame[word].word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1938c52-8e42-4475-8ddb-c47c9a14ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_matrix_1, difference_matrix_2, difference_matrix_3 = [], [], []\n",
    "difference_matrix_4, difference_matrix_5, difference_matrix_6 = [], [], []\n",
    "\n",
    "difference_matrices = [difference_matrix_1, difference_matrix_2, \n",
    "                       difference_matrix_3, difference_matrix_4, \n",
    "                       difference_matrix_5, difference_matrix_6]\n",
    "\n",
    "for i, matrix in enumerate(syl_frames):\n",
    "    for row_word in matrix.values():\n",
    "        row = np.array([])\n",
    "        for column_word in matrix.values():\n",
    "            # calculate the frob norm for each word pair and putting it into diff matrix \n",
    "            difference_matrix = row_word.subtract(column_word).to_numpy()\n",
    "            frob_norm = linalg.norm(difference_matrix)\n",
    "\n",
    "            row = np.append(row, frob_norm)\n",
    "\n",
    "        difference_matrices[i].append(row)\n",
    "    \n",
    "    # turn into correlation matrix \n",
    "    diff = np.array([difference_matrices[i]])\n",
    "    df = pd.DataFrame(diff[0])\n",
    "    df.columns, df.index = labels[i], labels[i]\n",
    "    correlations = df.corr()\n",
    "    correlations_array = np.asarray(df.corr())\n",
    "    \n",
    "    # replace the uncondensed matrix with the condensed one\n",
    "    difference_matrices[i] = correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8c69cf1-eacb-4e28-90bf-b152bb645ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Levenshtein_matrix_1, Levenshtein_matrix_2, Levenshtein_matrix_3 = [], [], []\n",
    "Levenshtein_matrix_4, Levenshtein_matrix_5, Levenshtein_matrix_6 = [], [], []\n",
    "\n",
    "Levenshtein_matrices = [Levenshtein_matrix_1, Levenshtein_matrix_2, \n",
    "                       Levenshtein_matrix_3, Levenshtein_matrix_4, \n",
    "                       Levenshtein_matrix_5, Levenshtein_matrix_6]\n",
    "\n",
    "for i, arr in enumerate(labels):\n",
    "    for row_word in labels[i]:\n",
    "        row = np.array([])\n",
    "        for column_word in labels[i]:\n",
    "            lev = Levenshtein.distance(row_word, column_word)\n",
    "            row = np.append(row, lev)\n",
    "\n",
    "        Levenshtein_matrices[i].append(row)\n",
    "        \n",
    "    # turn into correlation matrix \n",
    "    diff = np.array([Levenshtein_matrices[i]])\n",
    "    df = pd.DataFrame(diff[0])\n",
    "    df.columns, df.index = labels[i], labels[i]\n",
    "    correlations = df.corr()\n",
    "    correlations_array = np.asarray(df.corr())\n",
    "    \n",
    "    # replace the uncondensed matrix with the condensed one\n",
    "    Levenshtein_matrices[i] = correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2da1d4-a3c7-483a-9874-bda847f111d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(Levenshtein_matrices[4], cmap='vlag', linewidths=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29264e-a0fc-49b7-ad25-31e4885ba38a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
